{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31352dfb",
   "metadata": {},
   "source": [
    "## Lab 4 - MNIST\n",
    "\n",
    "For lab 4, you will complete and submit this ipynb to Canvas by the deadline.\n",
    "\n",
    "Please see the posted Fashion-MNIST Jupyter Notebook. You should use it as the guideline to complete this Jupyter Notebook. There are many similarities, except a few places that you need to make modifications.\n",
    "\n",
    "You will import the required modules, download the training/test data, use the dataloader, train the model (aim for at least 70% accuracy), and then use the model to test a few handwritten digits from the test dataset.\n",
    "\n",
    "Every cell in this file must correspond to the similar cell in the same order in the fashion-MNIST file provided above. There is one optional cell at the every end, in case you would like to use it for more visualization, testing ... \n",
    "\n",
    "Add a comment to every line of code, and code segment (e.g. cell, branching, repetition, function, ...) of your code. Please refer to the pytorch documentation. This is meant to demonstrate some reasonable understanding of what is happing in the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "73730f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the needed imports\n",
    "import torch # the pytorch library, includes various useful functions\n",
    "from torch import nn # import neural network from pytorch\n",
    "from torch.utils.data import DataLoader # import the data loader utility from pytorch\n",
    "from torchvision import datasets # import the datasets from torchvision (where we will use one of the datasets, MNIST)\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose # import various useful transformation functions from torchvision\n",
    "import matplotlib.pyplot as plt # import matplotlib for plotting graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f69ff374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST( # load data from MNIST dataset\n",
    "    root=\"data\", # the root for folder is data\n",
    "    train=True, # want training data\n",
    "    download=True, # want to download data\n",
    "    transform=ToTensor(), # convert data to tensor form\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST( # load data from MNIST dataset\n",
    "    root=\"data\", # the root for folder is data\n",
    "    train=False, # want nontraining/test data\n",
    "    download=True, # want to download data\n",
    "    transform=ToTensor(), # convert data to tensor form\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a5405f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "\n",
    "# of training examples used in one iteration\n",
    "batch_size = 64\n",
    "\n",
    "# sets up the dataloaders for train data\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "# sets up the dataloaders for test data\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# displays the dimensions of the data\n",
    "for X, y in test_dataloader:\n",
    "    # displays the shape of the input, [number of samples, types of pixels (1 for black and white), height, width]\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    # displays the number of samples and batch size of the data\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    # once we know shape of entry, we can assume same dimensions for rest\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7510e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "# if device supports cude, use it, otherwise use cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# displays the selected device\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "# Define an NN model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # NN constructor\n",
    "    def __init__(self):\n",
    "        # creates a model based on the basic neural network model\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # defines the flatten function, which flattens to -1 for easier usage\n",
    "        self.flatten = nn.Flatten()\n",
    "        # defines the dimensions of the NN, and what layers it passes through\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512), # linear transformation used first, with our 28x28 image (so 28x28 input size) and 512 output size\n",
    "            nn.ReLU(), # applies a element-wise rectified linear unit function\n",
    "            nn.Linear(512, 512), # add linear transformation used first, with our 28x28 image (so 28x28 input size) and 512 output size\n",
    "            nn.ReLU(), # applies another element-wise rectified linear unit function\n",
    "            nn.Linear(512, 10) # add linear transformation used first, with our 28x28 image (so 28x28 input size) and 10 output size (for the 10 numbers 0-9)\n",
    "        )\n",
    "    # model forwarding method\n",
    "    def forward(self, x):\n",
    "        # performs previously defined flatten function\n",
    "        x = self.flatten(x)\n",
    "        # performs previously defined 'type of model' function\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        # returns the model\n",
    "        return logits\n",
    "# sends the model to the best device, where cuda is preferred over cpu\n",
    "model = NeuralNetwork().to(device)\n",
    "# prints how the model looks like\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "52632ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss and optimizer to be used in the next next cell\n",
    "# defines the loss as a cross-entropy function - which builds upon entropy and generally calculates the difference between two probability distributions\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# defines the optimizer as a stochastic gradient descent, where the learning rate is 1e-3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6b05ba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train and test functions\n",
    "# the train function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    # gets the size of the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # trains the model\n",
    "    model.train()\n",
    "    # for statement that cycles through each tensor in the dataloader\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # sends the tensor to the device for computation\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        # tells model that it is going to be trained\n",
    "        pred = model(X)\n",
    "        # uses the prediction error to calculate the cross entropy loss\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        # sets the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # computes gradeint of tensor with respect to graph leaves\n",
    "        loss.backward()\n",
    "        # performs parameter update based on current gradient and the update rule\n",
    "        optimizer.step()\n",
    "\n",
    "        # performs this function each time batch trains 100 images\n",
    "        if batch % 100 == 0:\n",
    "            # gets the current loss, gets the current batch length number * size of batch (in our case 64)\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            # prints out the data in a formatted fashion\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# the test function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    # gets the size of the dataset\n",
    "    size = len(dataloader.dataset)\n",
    "    # gets the number of batches\n",
    "    num_batches = len(dataloader)\n",
    "    # prepares the model for evaluation\n",
    "    model.eval()\n",
    "    # initilizes variables\n",
    "    test_loss, correct = 0, 0\n",
    "    # does this part without gradiant calculations\n",
    "    with torch.no_grad():\n",
    "        # for loop to iterate thorugh data entries\n",
    "        for X, y in dataloader:\n",
    "            # sends the data to the device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # predicts each image\n",
    "            pred = model(X)\n",
    "            # adds the loss function's computation to the total loss\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # addes the computed correctness to the total amount of correctness\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # computes the average loss\n",
    "    test_loss /= num_batches\n",
    "    # computes the average correctness\n",
    "    correct /= size\n",
    "    # prints out formatted information\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0b730117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298764  [    0/60000]\n",
      "loss: 2.302129  [ 6400/60000]\n",
      "loss: 2.281877  [12800/60000]\n",
      "loss: 2.283576  [19200/60000]\n",
      "loss: 2.275968  [25600/60000]\n",
      "loss: 2.278544  [32000/60000]\n",
      "loss: 2.272368  [38400/60000]\n",
      "loss: 2.274262  [44800/60000]\n",
      "loss: 2.265929  [51200/60000]\n",
      "loss: 2.249832  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 2.251803 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.252370  [    0/60000]\n",
      "loss: 2.251752  [ 6400/60000]\n",
      "loss: 2.240843  [12800/60000]\n",
      "loss: 2.221786  [19200/60000]\n",
      "loss: 2.226598  [25600/60000]\n",
      "loss: 2.228179  [32000/60000]\n",
      "loss: 2.211349  [38400/60000]\n",
      "loss: 2.224940  [44800/60000]\n",
      "loss: 2.202334  [51200/60000]\n",
      "loss: 2.181723  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.7%, Avg loss: 2.183801 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.183800  [    0/60000]\n",
      "loss: 2.177172  [ 6400/60000]\n",
      "loss: 2.176991  [12800/60000]\n",
      "loss: 2.127260  [19200/60000]\n",
      "loss: 2.147084  [25600/60000]\n",
      "loss: 2.144969  [32000/60000]\n",
      "loss: 2.109441  [38400/60000]\n",
      "loss: 2.140523  [44800/60000]\n",
      "loss: 2.096543  [51200/60000]\n",
      "loss: 2.065781  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 2.067028 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.066497  [    0/60000]\n",
      "loss: 2.047332  [ 6400/60000]\n",
      "loss: 2.063373  [12800/60000]\n",
      "loss: 1.966350  [19200/60000]\n",
      "loss: 2.004902  [25600/60000]\n",
      "loss: 1.994460  [32000/60000]\n",
      "loss: 1.932993  [38400/60000]\n",
      "loss: 1.990099  [44800/60000]\n",
      "loss: 1.912984  [51200/60000]\n",
      "loss: 1.865540  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 1.863711 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.866800  [    0/60000]\n",
      "loss: 1.825609  [ 6400/60000]\n",
      "loss: 1.859980  [12800/60000]\n",
      "loss: 1.708202  [19200/60000]\n",
      "loss: 1.760365  [25600/60000]\n",
      "loss: 1.736877  [32000/60000]\n",
      "loss: 1.656885  [38400/60000]\n",
      "loss: 1.745649  [44800/60000]\n",
      "loss: 1.632531  [51200/60000]\n",
      "loss: 1.570292  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 1.561046 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.580847  [    0/60000]\n",
      "loss: 1.506901  [ 6400/60000]\n",
      "loss: 1.556616  [12800/60000]\n",
      "loss: 1.380186  [19200/60000]\n",
      "loss: 1.429596  [25600/60000]\n",
      "loss: 1.397116  [32000/60000]\n",
      "loss: 1.322925  [38400/60000]\n",
      "loss: 1.440055  [44800/60000]\n",
      "loss: 1.318857  [51200/60000]\n",
      "loss: 1.250823  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 1.234912 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.283153  [    0/60000]\n",
      "loss: 1.181405  [ 6400/60000]\n",
      "loss: 1.235679  [12800/60000]\n",
      "loss: 1.086970  [19200/60000]\n",
      "loss: 1.124077  [25600/60000]\n",
      "loss: 1.088995  [32000/60000]\n",
      "loss: 1.036102  [38400/60000]\n",
      "loss: 1.165047  [44800/60000]\n",
      "loss: 1.072204  [51200/60000]\n",
      "loss: 1.005204  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.5%, Avg loss: 0.984753 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.058144  [    0/60000]\n",
      "loss: 0.943226  [ 6400/60000]\n",
      "loss: 0.988234  [12800/60000]\n",
      "loss: 0.886560  [19200/60000]\n",
      "loss: 0.913507  [25600/60000]\n",
      "loss: 0.874969  [32000/60000]\n",
      "loss: 0.838533  [38400/60000]\n",
      "loss: 0.967762  [44800/60000]\n",
      "loss: 0.908658  [51200/60000]\n",
      "loss: 0.846634  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.820092 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.908075  [    0/60000]\n",
      "loss: 0.786931  [ 6400/60000]\n",
      "loss: 0.821087  [12800/60000]\n",
      "loss: 0.758581  [19200/60000]\n",
      "loss: 0.777618  [25600/60000]\n",
      "loss: 0.736052  [32000/60000]\n",
      "loss: 0.706445  [38400/60000]\n",
      "loss: 0.834701  [44800/60000]\n",
      "loss: 0.799771  [51200/60000]\n",
      "loss: 0.746329  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.711351 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.805949  [    0/60000]\n",
      "loss: 0.680302  [ 6400/60000]\n",
      "loss: 0.707723  [12800/60000]\n",
      "loss: 0.674348  [19200/60000]\n",
      "loss: 0.684882  [25600/60000]\n",
      "loss: 0.643919  [32000/60000]\n",
      "loss: 0.613641  [38400/60000]\n",
      "loss: 0.743512  [44800/60000]\n",
      "loss: 0.722067  [51200/60000]\n",
      "loss: 0.680053  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.635736 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Training the NN model \n",
    "# of passes of the entire training dataset that algorithm has completed\n",
    "epochs = 10 # starting with 5 epochs --> may need to adjust\n",
    "# iterate epoch number of times\n",
    "for t in range(epochs):\n",
    "    # print header for easier legibility\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    # trains the model on train data\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    # test the model on test data\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "# print when epoch is done\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8b59ba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "# Saving the model in a file, we will use it in the next cell\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "# print that model has been saved\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bf6ddf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"7\", Actual: \"7\"\n",
      "Predicted: \"2\", Actual: \"2\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"9\", Actual: \"4\"\n",
      "Predicted: \"4\", Actual: \"9\"\n",
      "Predicted: \"6\", Actual: \"5\"\n",
      "Predicted: \"9\", Actual: \"9\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"2\", Actual: \"6\"\n",
      "Predicted: \"9\", Actual: \"9\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"1\", Actual: \"1\"\n",
      "Predicted: \"3\", Actual: \"5\"\n",
      "Predicted: \"9\", Actual: \"9\"\n",
      "Predicted: \"7\", Actual: \"7\"\n",
      "Predicted: \"3\", Actual: \"3\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"7\", Actual: \"9\"\n",
      "Predicted: \"6\", Actual: \"6\"\n",
      "Predicted: \"6\", Actual: \"6\"\n",
      "Predicted: \"5\", Actual: \"5\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"7\", Actual: \"7\"\n",
      "Predicted: \"4\", Actual: \"4\"\n",
      "Predicted: \"0\", Actual: \"0\"\n",
      "Predicted: \"1\", Actual: \"1\"\n"
     ]
    }
   ],
   "source": [
    "# Using the model for some testing\n",
    "\n",
    "# Test for at least three test cases\n",
    "# creates new model of type Neural Network\n",
    "model = NeuralNetwork()\n",
    "# loades up the saved model\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "# declare the number of classes of the model, where # of classes equal to the output size\n",
    "classes = [\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\"\n",
    "]\n",
    "\n",
    "# sets up the model for evaluation\n",
    "model.eval()\n",
    "# iterate through first 30 test data entries\n",
    "for i in range(30):\n",
    "    # gets the image and expected predicted class\n",
    "    x, y = test_data[i][0], test_data[i][1]\n",
    "    # does this part without gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # applies the model to x, to get a prediction\n",
    "        pred = model(x)\n",
    "        # translate the actual/prediction to the proper classes\n",
    "        predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "        # prints human-readable printout\n",
    "        print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "06b43a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dced63a700>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the test images used in the previous cell\n",
    "plt.imshow(test_data[7][0].squeeze(), cmap = \"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7e75b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May include more testing and visualization here (optional)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
